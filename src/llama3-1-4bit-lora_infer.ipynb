{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-05T08:32:26.509026Z","iopub.status.busy":"2024-08-05T08:32:26.508716Z","iopub.status.idle":"2024-08-05T08:32:56.362036Z","shell.execute_reply":"2024-08-05T08:32:56.360676Z","shell.execute_reply.started":"2024-08-05T08:32:26.508999Z"},"trusted":true},"outputs":[],"source":["!pip install transformers peft accelerate bitsandbytes \\\n","    -qU --no-index --find-links /kaggle/input/lmsys-packages"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-05T08:32:56.365148Z","iopub.status.busy":"2024-08-05T08:32:56.364747Z","iopub.status.idle":"2024-08-05T08:32:56.372301Z","shell.execute_reply":"2024-08-05T08:32:56.371174Z","shell.execute_reply.started":"2024-08-05T08:32:56.365113Z"},"trusted":true},"outputs":[],"source":["SUBMIT_KAGGLE = True\n","MULTI_GPU = True\n","DO_EVAL = False\n","\n","MAX_TOKENS = 2800\n","MAX_PROMPT_TOKENS = 512\n","MAX_RESPONSE_TOKENS = 1024\n","BATCH_SIZE = 2\n","\n","TEMPLATE = (\n","    \"Given a prompt and two responses #a and #b, evaluate which response is superior or if both responses \"\n","    \"are equally good.\\n<Prompt>:<|reserved_special_token_50|>\\n{prompt}\\n<|reserved_special_token_51|>\"\n","    \"\\n\\n<Response #a>:\\n<|reserved_special_token_52|>\\n{resp_a}\\n<|reserved_special_token_53|>\"\n","    \"\\n\\n<Response #b>:\\n<|reserved_special_token_54|>\\n{resp_b}\\n<|reserved_special_token_55|>\"\n","    \"\\n\\nEvaluate which response is superior or if both responses are equally good. \"\n","    \"Answer with a, b, or tie.\\n### Answer:\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-05T08:32:56.374232Z","iopub.status.busy":"2024-08-05T08:32:56.373834Z","iopub.status.idle":"2024-08-05T08:32:56.381856Z","shell.execute_reply":"2024-08-05T08:32:56.380946Z","shell.execute_reply.started":"2024-08-05T08:32:56.374202Z"},"trusted":true},"outputs":[],"source":["if SUBMIT_KAGGLE:\n","    MODEL_PATH = \"/kaggle/input/llama3-1-8b-4bit-unsloth\"\n","    ADAPTER_PATH = \"/kaggle/input/llama3-1-8b-4bit-lora-lmsys-aug-length-3\"\n","    TRAIN_DATA_PATH = \"/kaggle/input/lmsys-chatbot-arena/train.csv\"\n","    TEST_DATA_PATH = \"/kaggle/input/lmsys-chatbot-arena/test.csv\"\n","else:\n","    MULTI_GPU = False\n","    MODEL_PATH = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n","    ADAPTER_PATH = \"../saves/lmsys/llama3.1-8B_lora_sft/checkpoint-3592\"\n","    TRAIN_DATA_PATH = \"../data/csv/train.csv\"\n","    TEST_DATA_PATH = \"../data/csv/test.csv\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-05T08:32:56.387509Z","iopub.status.busy":"2024-08-05T08:32:56.386978Z","iopub.status.idle":"2024-08-05T08:33:13.314277Z","shell.execute_reply":"2024-08-05T08:33:13.313166Z","shell.execute_reply.started":"2024-08-05T08:32:56.387480Z"},"trusted":true},"outputs":[],"source":["import os\n","import torch\n","import numpy as np\n","import pandas as pd\n","import torch.nn as nn\n","from concurrent.futures import ThreadPoolExecutor\n","from datasets import Dataset\n","from sklearn.model_selection import train_test_split\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from peft import PeftModel\n","from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n","from tqdm import tqdm\n","\n","torch.backends.cuda.enable_mem_efficient_sdp(True)\n","torch.backends.cuda.enable_flash_sdp(True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-05T08:33:13.316189Z","iopub.status.busy":"2024-08-05T08:33:13.315628Z","iopub.status.idle":"2024-08-05T08:33:13.327003Z","shell.execute_reply":"2024-08-05T08:33:13.326143Z","shell.execute_reply.started":"2024-08-05T08:33:13.316163Z"},"trusted":true},"outputs":[],"source":["def process_text(text):\n","    text = \" \".join(eval(text, {\"null\": \"None\"}))\n","    return text\n","\n","\n","def tokenize(sample, tokenizer, prompt_template, max_tokens, max_prompt_tokens, max_resp_tokens):\n","    prompt_text = process_text(sample[\"prompt\"])\n","    resp_a_text = process_text(sample[\"response_a\"])\n","    resp_b_text = process_text(sample[\"response_b\"])\n","    \n","    encoded_prompt = tokenizer.encode(prompt_text)\n","    encoded_resp_a = tokenizer.encode(resp_a_text)\n","    encoded_resp_b = tokenizer.encode(resp_b_text)\n","    encoded_input_text = encoded_prompt + encoded_resp_a + encoded_resp_b\n","    if len(encoded_input_text) > max_tokens:\n","        input_text = prompt_template.format_map({\n","            \"prompt\": tokenizer.decode(encoded_prompt[:max_prompt_tokens]), \n","            \"resp_a\": tokenizer.decode(encoded_resp_a[:max_resp_tokens]),\n","            \"resp_b\": tokenizer.decode(encoded_resp_b[:max_resp_tokens])\n","        })\n","    else:\n","        input_text = prompt_template.format_map({\n","            \"prompt\": prompt_text, \n","            \"resp_a\": resp_a_text,\n","            \"resp_b\": resp_b_text\n","        })\n","\n","    label = -1\n","    if \"winner_model_a\" in sample:\n","        if sample[\"winner_model_a\"] == 1:\n","            label = 0\n","        elif sample[\"winner_model_b\"] == 1:\n","            label = 1\n","        else:\n","            label = 2\n","    \n","    input_text = tokenizer.apply_chat_template(\n","        [{\"role\": \"user\", \"content\": input_text}], tokenize=False, add_generation_prompt=True)\n","    encoded_text = tokenizer(input_text, add_special_tokens=False)\n","    return {\n","        **encoded_text,\n","        \"labels\": torch.tensor(label)\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-05T08:33:13.330092Z","iopub.status.busy":"2024-08-05T08:33:13.329823Z","iopub.status.idle":"2024-08-05T08:33:14.216813Z","shell.execute_reply":"2024-08-05T08:33:14.215726Z","shell.execute_reply.started":"2024-08-05T08:33:13.330069Z"},"trusted":true},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, token=os.environ.get(\"HF_TOKEN\", \"\"))\n","tokenizer.padding_side = \"right\"\n","\n","if DO_EVAL:\n","    data_df = pd.read_csv(TRAIN_DATA_PATH)\n","    _, test_df = train_test_split(data_df, test_size=500, random_state=42)\n","else:\n","    test_df = pd.read_csv(TEST_DATA_PATH)\n","\n","test_dataset = Dataset.from_pandas(test_df)\n","tokenized_datasets = test_dataset.map(\n","    tokenize, \n","    fn_kwargs={\"tokenizer\": tokenizer, \"prompt_template\": TEMPLATE, \"max_tokens\": MAX_TOKENS,\n","               \"max_prompt_tokens\": MAX_PROMPT_TOKENS, \"max_resp_tokens\": MAX_RESPONSE_TOKENS},\n",")\n","encoded_test_df = tokenized_datasets.to_pandas()\n","encoded_test_df[\"max_len\"] = encoded_test_df[\"input_ids\"].apply(len)\n","encoded_test_df = encoded_test_df.sort_values(\"max_len\", ascending=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-05T08:33:14.218367Z","iopub.status.busy":"2024-08-05T08:33:14.218063Z","iopub.status.idle":"2024-08-05T08:34:13.845201Z","shell.execute_reply":"2024-08-05T08:34:13.844356Z","shell.execute_reply.started":"2024-08-05T08:33:14.218340Z"},"trusted":true},"outputs":[],"source":["device_0 = torch.device(\"cuda:0\")\n","base_model_0 = AutoModelForCausalLM.from_pretrained(MODEL_PATH, device_map=\"cuda:0\")\n","model_0 = PeftModel.from_pretrained(base_model_0, model_id=ADAPTER_PATH).to(device_0) \n","model_0.eval()\n","if MULTI_GPU:\n","    device_1 = torch.device(\"cuda:1\")\n","    base_model_1 = AutoModelForCausalLM.from_pretrained(MODEL_PATH, device_map=\"cuda:1\")\n","    model_1 = PeftModel.from_pretrained(base_model_1, model_id=ADAPTER_PATH).to(device_1) \n","    model_1.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-05T08:34:13.846579Z","iopub.status.busy":"2024-08-05T08:34:13.846289Z","iopub.status.idle":"2024-08-05T08:34:13.865324Z","shell.execute_reply":"2024-08-05T08:34:13.864169Z","shell.execute_reply.started":"2024-08-05T08:34:13.846553Z"},"trusted":true},"outputs":[],"source":["@torch.no_grad()\n","@torch.cuda.amp.autocast()\n","def inference(df, model, tokenizer, device, batch_size, do_eval=False):\n","    label_ids = [tokenizer(i, add_special_tokens=False)[\"input_ids\"][0] for i in ['a', 'b', 'tie']]\n","\n","    a_win, b_win, tie = [], [], []\n","    loss, acc = 0, 0\n","\n","    for start_idx in tqdm(range(0, len(df), batch_size)):\n","        end_idx = min(start_idx + batch_size, len(df))\n","        tmp = df.iloc[start_idx: end_idx]\n","        input_ids = tmp[\"input_ids\"].to_list()\n","        attention_mask = tmp[\"attention_mask\"].to_list()\n","        if DO_EVAL:\n","            inputs = pad_without_fast_tokenizer_warning(\n","                tokenizer,\n","                {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n","                padding=\"max_length\",\n","                max_length=MAX_TOKENS,\n","                pad_to_multiple_of=None,\n","                return_tensors=\"pt\",\n","            )\n","        else:\n","            inputs = pad_without_fast_tokenizer_warning(\n","                tokenizer,\n","                {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n","                padding=\"longest\",\n","                pad_to_multiple_of=None,\n","                return_tensors=\"pt\",\n","            )\n","            \n","        input_ids = inputs[\"input_ids\"].to(device)\n","        attention_mask = inputs[\"attention_mask\"].to(device)\n","        input_last_idx = torch.sum(inputs[\"attention_mask\"], dim=-1) - 1\n","\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits[torch.arange(outputs.logits.shape[0]), input_last_idx]\n","        logits = logits[:, label_ids]\n","        preds = torch.argmax(logits, dim=-1)\n","\n","        if do_eval:\n","            labels = torch.tensor(tmp[\"labels\"].to_list(), device=device)\n","            loss += nn.CrossEntropyLoss(reduction=\"sum\")(logits, labels)\n","            acc += torch.sum(preds == labels)\n","\n","        proba = torch.softmax(logits, dim=-1).cpu().numpy()\n","        prob_a_win = proba[:, 0].tolist()\n","        prob_b_win = proba[:, 1].tolist()\n","        prob_tie = proba[:, 2].tolist()\n","\n","        a_win.extend(prob_a_win)\n","        b_win.extend(prob_b_win)\n","        tie.extend(prob_tie)\n","\n","    df[\"winner_model_a\"] = a_win\n","    df[\"winner_model_b\"] = b_win\n","    df[\"winner_tie\"] = tie\n","\n","    if do_eval:\n","        loss = loss.item() / len(df)\n","        acc = acc.item() / len(df)\n","        print(loss, acc)\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-05T08:34:13.866801Z","iopub.status.busy":"2024-08-05T08:34:13.866531Z","iopub.status.idle":"2024-08-05T08:34:18.103696Z","shell.execute_reply":"2024-08-05T08:34:18.102790Z","shell.execute_reply.started":"2024-08-05T08:34:13.866778Z"},"trusted":true},"outputs":[],"source":["if MULTI_GPU:\n","    sub_1 = encoded_test_df.iloc[0::2].copy()\n","    sub_2 = encoded_test_df.iloc[1::2].copy()\n","\n","    with ThreadPoolExecutor(max_workers=2) as executor:\n","        results = executor.map(\n","            inference, (sub_1, sub_2), (model_0, model_1), (tokenizer, tokenizer),\n","            (device_0, device_1), (BATCH_SIZE, BATCH_SIZE))\n","    result_df = pd.concat(list(results), axis=0)\n","    proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n","\n","    result_df.loc[:, \"winner_model_a\"] = proba[:, 0]\n","    result_df.loc[:, \"winner_model_b\"] = proba[:, 1]\n","    result_df.loc[:, \"winner_tie\"] = proba[:, 2]\n","    submission_df = result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]\n","else:\n","    submission_df = inference(encoded_test_df, model_0, tokenize, device_0, BATCH_SIZE, DO_EVAL)\n","\n","submission_df.to_csv('submission.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-08-05T08:34:18.106656Z","iopub.status.busy":"2024-08-05T08:34:18.106289Z","iopub.status.idle":"2024-08-05T08:34:18.123097Z","shell.execute_reply":"2024-08-05T08:34:18.122227Z","shell.execute_reply.started":"2024-08-05T08:34:18.106625Z"},"trusted":true},"outputs":[],"source":["display(submission_df)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":8346466,"sourceId":66631,"sourceType":"competition"},{"datasetId":5460937,"sourceId":9056640,"sourceType":"datasetVersion"},{"datasetId":5487254,"sourceId":9093052,"sourceType":"datasetVersion"},{"datasetId":5499994,"sourceId":9112323,"sourceType":"datasetVersion"}],"dockerImageVersionId":30747,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
